# Log Structured Merge

NoSQL databases like Cassandra have exploded in popularity in recent years. One driver is the insatiable demand to ingest huge amounts of data from an ever-growing number of sources like mobile applications and IoT devices. 

The secret sauce behind many of these NoSQL databases is a data structure called the Log Structured Merge tree. An LSM tree is optimized for fast writes. To understand how an LSM tree works, let's first take a look at how data is typically stored in a relational database. 

A relational database is commonly backed by a data structure called B-tree. B-tree is optimized for reads. Updating the B-tree is relatively expensive as it involves random IO and might include updating multiple pages on disk. This limits how fast a B-tree can ingest data. 

An LSM tree works differently. Writes are batched in memory as they arrived in a structure called a memtable. A memtable is ordered by object key and is usually implemented as a balanced binary tree. As a memtable reaches a certain size, it is flushed to disk as an immutable Sorted String Table. An SSTable stores the key-value pairs in a sorted sequence. These writes are all sequential IO. They are fast on any storage media. The new SSTable becomes the most recent segment of the LSM tree. As more data comes in, more and more of these immutable SSTables are created and added to the LSM tree, with each one representing a small chronological subset of the incoming changes. Since SSTables are immutable, an update to an existing object key does not overwrite an existing SSTable. Instead, a new entry is added to the most recent SSTable which supersedes any entries in the old SSTables for the object key. Deleting an object requires special handling too, since we cannot mark anything in the SSTable as deleted. To perform a delete, it adds a marker called a tombstone to the most recent SSTable for the object key. When we encounter the tombstone on read, we know the object has been deleted. And yes, it is a bit unintuitive that a delete takes up extra space. 

To serve a read, we first try to find the key in the memtable, then in the most recent SSTable in the LSM tree, then the next SSTable, and so on. Since an SSTable is sorted, the lookup can be one efficiently. The accumulation of SSTables presents two issues. As the number of SSTables grows, it would take an increasingly long time to look up a key. As the SSTables accumulate, there are more and more outdated entries as keys are updated and tombstones are added. These take up previous disk space. 

To fix these issues, there is a periodic merging and compaction process running in the background to merge SSTables and discard outdated or deleted values. This reclaims disk space and caps the number of SSTables a read has to look though. Since the SSTables are sorted, this merging and compaction process is simple and efficient. The approach is similar to the one used in the merge phase of the merge sort algorithm. This in a nutshell is how an LSM tree provides fast writes. 

Let's recap. An LSM tree buffers incoming writes in memory. hen the buffer fills up, we sort and flush it to disk as an immutable SSTable. The number of SSTables increases as more buffers get flushed to disk. This creates a problem for reads as each read has to search though these SSTables to perform a lookup. To cap the number of SSTables it has to search though for each read, the LSM tree merges the SSTables and compacts them in the background. 

Let's look at compaction more closely. When the SSTables are merged, they are organize into levels. This is where the tree part of the LSM tree comes into the picture. There are different strategies to determine when and how the SSTables are merged and compacted. There are two broad strategies. Size Tiered Compaction and Leveled Compaction. Size Tiered compaction is optimized for write throughput. Leveled compaction is more read-optimized. Compaction keeps the number of SSTables manageable. The SSTables are organize into levels. Each level gets exponentially larger as SSTables from the level above are merged into it. Compaction consumes a lot of I/O. A mistuned compaction could starve the system and slow down both read and write. 

Finally, let's visit some common optimizations for the LSM trees in production systems. There are many optimization strategies that try to provide read performance closer to that of the B-tree. What we explained below are just the common ones, and they are by no means exhaustive. To look up a key, it performs searches on the SSTables at every level. Even though search is fast on sorted data, going though all the on-disk SSTables consumes a lot of I/O. Many systems keep a summary table in memory that contains the min/max range of each disk block of every level. It allows the system to skip searches on these disk blocks where the key does not fall within the range. This saves a lot of random IO. Another issue that could be potentially expensive is to look up a key that does not exist. This would require looking through all the eligible blocks in all the levels. Most systems keep a bloom filter at each level. A bloom filter is a space-efficient data structure that returns a firm no if the key does not exist, and a "probably yes" if a key might exist. This allows the system to skip a level entirely if the key does not exist there, which dramatically reduces the number of random I/O required. 

In conclusion, NoSQL databases backed by an LSM tree could be tuned to support a very high write rate. As with any database, proper tuning is the key. For an LSM tree, compaction tuning is the most critical. 