# How to pick the right database?
When building a highly scalable system, choosing the right database is often the single most important decision we'll ever make. If we are fortunate enough to be tasked with the challenge of picking the next database for our super-fast growing business, here are some key points to consider. We are talking about a database for a real growing business, where a bad choice would lead to extended downtime, customer impact and even data loss. We are not picking something for our weekend project. This take is probably a bit controversial. Let's dive right in. 

First, are we positive that we need a different database? There is a compelling reason we started to look for an alternative, right? It the existing database breaking at the seams? Maybe the p95 latency is through the roof. Maybe the working set is overflowing the available memory, and even the most basic requests need to go the the disk and slow everything down. Whatever the issues are, make sure they are not easily solvable. Let's read the database manual of our current database system - front to back -and read it again. There could be a configuration knob or two that we can tweak to give us a bit more breathing room. This breathing room could come in handy, because migrating a database could take a long time, usually much longer than we think. These knobs could come in the form of tunning the working set memory size, choosing a different compaction strategy or even changing some garbage collection behavior. Databases are complex and highly tunable. Understand the architecture of our database. Know its limitations. Reach out to experts in the community. Describe our problems. People in the know could help, and often in surprising ways. To look for more untapped headroom, maybe there are some fixes to our application architecture that would give us more breathing room. Can we put a cache in front of it, and give us a few more months of runway? Can we add read replicas to offload some read load? Can we shard the database, or partition the data in some way? Maybe the data is naturally siloed, and sharding is an acceptable solution. The bottom line is this: Migrating live production database is risky and costly. We better be damn sure that there is no way to keep using the current database. 

Okay, we have exhausted all avenues for the current database. How do we go about choosing the next one? We, developers, are naturally drawn to the new and shiny, like months to a flame. When it comes to databases, though, boring is good. We should prefer the ones that have been around for a long time and have been battle tested. Depending on the industry we are in, our posture could be a little different. Banking and finance are a lot more conservative, for example. Whatever it is, there should be a ready market of experienced administrators and developers for the database we are considering. Software engineering at scale is about tradeoffs. When it comes to databases, it is even more true. Keep in mind there is no free lunch. Be weary of outrageous marketing claims. Infinite, effortless horizontal scalability comes with a hidden cost. Dig deep to fin where that cost is hiding. Instead of reading the shiny brochures, go read the manual. There is usually a page called "Limits". The page is a gem. The FAQ section is also very useful. These pages in the manual are where we learn the real limits of a new database. Its design constraints - the fine prints so to speak. If the brochure promised infinite horizontal scalability, These are the places to fact-check those claims and to find the catch. In our experience, the fancier the claims, the longer the disclaimers are in the back. For example, many NoSQL databases support much higher scale than the trusty old relational databases. They often claim to support near linearly horizontal scalability. Here are the common trade-offs that come with them. One, they eliminate or limit transactional guarantees. Two, they severely limit data modeling flexibility. There are no queries across data entities. The data is highly denormalized  where the same piece of data is stored in many collections to support different data access patterns. For open source projects, read the GitHub issues. Learn as much as possible about the candidate now. The investment is relatively small at this juncture.

Once we narrow down the database options, what's next? Well, let's have a shoot-out. Create a realistic test bench for the candidates using our own data, with our own real-world access patterns. We have that data, right? After all, our current database is bursting at the seams, so getting some representative dataset should be possible. Yes, this is costly, and it could take weeks. But we cannot afford to skip this step. Migrating a production database is risky, and it takes a lot more work than benchmarking. If we are staking our career on this new database, let's make sure it will work. During benchmarking, pay attention to the outliers. Measure P99 of everything. The average is not meaningful. Try hard to replicate the real workload, and then push it further and see where it starts to break. Try some of the more risky operational tasks - like failing over a node, or testing for data corruption during network partitions. Try up- or down-sharding, if it is applicable. After everything checks out, plan the migration carefully. Write out a detailed step-by-step migration plan. Have your peers review it thoroughly. If possible, migrate a small service first, and learn as much as we can from that. Picking the right database is not glamorous, and there is a lot of hard work involved. Migrating to a new database in the real world could literally take years at a high scale. 